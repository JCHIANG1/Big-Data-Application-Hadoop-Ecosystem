1. create a 3-node cluster with medinum size (with a total of 1 master + 2 worker nodes). 

2. changing the default 8G hard drive to 30G on all nodes.

3. change the security group setting to open firewall access. (Right click on the security group and choose Edit inbound rules on AWS interface)
a) default (port 22 is required for regular SSH connections)
b) open port 50070 for the web interface to see the cluster status in a browser. 
c) set 0-64000 range opening up all ports to ensure that the ports are open only within the cluster and not to the world.
(assuming that all of your nodes in the cluster share the same security group – that will happen automatically if use the “create more like this” option when creating instances)

4. After setting the master node, right click on the master node and choose “create more like this” to create 2 more nodes with same settings. 
If configure the network settings on master first, security group information will be copied. 
NOTE: Hard drive size will not be copied and default to 8G unless you change it.

5.Connect to the master and set up Hadoop once on the master node, 
# install ant to list java processes
sudo yum install ant

#download files to the master instance from a URL link
wget http://rasinsrv07.cstcis.cti.depaul.edu/CSC555/hadoop-2.6.4.tar.gz

#unpack the archive
tar xzf hadoop-2.6.4.tar.gz

#determine the correct Java configuration line by executing 
readlink -f $(which java)
#/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.191.b12-0.amzn2.x86_64/jre/bin/java

#modify the hadoop-env.sh to add the correct java configuration
nano hadoop-2.6.4/etc/hadoop/hadoop-env.sh
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.191.b12-0.amzn2.x86_64

#modify the .bashrc file to add these two lines, the .bashrc file contains enviorment settings to be configured
automatically on each login
nano ~.bashrc
export HADOOP_HOME=~/hadoop-2.6.4
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

#to immediately refresh the settings (that will be automatic on next login)
source ~/.bashrc

#edit the configuration file
a)configure core-site.xml adding the privateIP
nano hadoop-2.6.4/etc/hadoop/core-site.xml
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://172.31.7.201/</value>
</property>

b)configure hdfs-site and set replication factor to 2
nano hadoop-2.6.4/etc/hadoop/hdfs-site.xml
<property>
 <name>dfs.replication</name>
 <value>2</value>
</property>

c)mapred-site.xml
#if mapred-site.xml file is not there, run the following single line command to create it 
by copying from template.
#cp hadoop-2.6.4/etc/hadoop/mapred-site.xml.template  hadoop-2.6.4/etc/hadoop/mapred-site.xml
nano hadoop-2.6.4/etc/hadoop/mapred-site.xml
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>

d)yarn-site.xml, user the privateIP of the master
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>172.31.7.201</value>
</property>

<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
